{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4b61b2",
   "metadata": {},
   "source": [
    "# Extended BUMP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c13c571",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143677da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3230cf",
   "metadata": {},
   "source": [
    "#### Configure plots look and feel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plot_theme(base_font: int) -> None:\n",
    "    \"\"\" \n",
    "    Set the theme for plots in the notebook.\n",
    "    \"\"\"\n",
    "    sns.set_theme(\n",
    "        style=\"whitegrid\", \n",
    "        context=\"talk\", \n",
    "        palette=\"colorblind\",\n",
    "        rc={\n",
    "            \"font.size\": base_font, \n",
    "            \"axes.titlesize\": base_font + 4, \n",
    "            \"axes.labelsize\": base_font + 2,\n",
    "            \"xtick.labelsize\": base_font,\n",
    "            \"ytick.labelsize\": base_font,\n",
    "            \"legend.fontsize\": base_font\n",
    "            })\n",
    "    \n",
    "    mpl.rcParams.update({\n",
    "        \"figure.titlesize\": base_font + 4,\n",
    "        \"axes.titlepad\": 30,\n",
    "        \"axes.labelpad\": 10,\n",
    "        })\n",
    "\n",
    "set_plot_theme(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3571a6e",
   "metadata": {},
   "source": [
    "#### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/bc_type_distribution_full_data.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a585891",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040819a0",
   "metadata": {},
   "source": [
    "#### Compute number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = len(df)\n",
    "print(f\"Total files: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c5cdf",
   "metadata": {},
   "source": [
    "#### Identify breaking and non-breaking files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca3f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_files = df[df[\"BC_kinds\"] != \"NONE\"]\n",
    "bc_count = len(bc_files)\n",
    "\n",
    "none_files = df[df[\"BC_kinds\"] == \"NONE\"]\n",
    "none_count = len(none_files)\n",
    "\n",
    "print(f\"Files with breaking changes: {bc_count} ({(bc_count / files) * 100:.2f}%)\")\n",
    "print(f\"Files with no breaking changes: {none_count} ({(none_count / files) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7275bab",
   "metadata": {},
   "source": [
    "#### Compute number of BC types per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f0803",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(\n",
    "    x=\"BC_kinds_count\",\n",
    "    data=df,\n",
    "    order=sorted(df[\"BC_kinds_count\"].unique())\n",
    ")\n",
    "\n",
    "for p in ax.patches:\n",
    "    p.set_alpha(0.8)\n",
    "    height = p.get_height() + 1\n",
    "    ax.annotate(f'{int(height)}',\n",
    "                (p.get_x() + p.get_width() / 2., height),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "ax.set_title(\"Breaking Change Types per File\")\n",
    "ax.set_xlabel(\"Breaking change type count\")\n",
    "ax.set_ylabel(\"File count\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bcs_labels(labels: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Parses the breaking change labels from a string and returns a list \n",
    "    of breaking change types (e.g., METHOD_REMOVED, METHOD_ADDED_TO_INTERFACE,\n",
    "    NONE). If the input is NaN or not a string, it returns an empty list.\n",
    "\n",
    "    :param labels: A string containing the breaking change labels separated \n",
    "        by semicolons.\n",
    "    :return: A list of breaking change types.\n",
    "    \"\"\"\n",
    "    if not pd.isna(labels) and isinstance(labels, str):\n",
    "        return [label.strip() for label in labels.split(\";\")]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bc_types\"] = df[\"BC_kinds\"].apply(parse_bcs_labels)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e05a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_label_bc_type_counts = Counter(df[\"bc_types\"].explode().tolist())\n",
    "single_label_counts = Counter([bc_types[0] for bc_types in df[\"bc_types\"] if len(bc_types) == 1])\n",
    "\n",
    "# Multi-label files\n",
    "bc_types_distribution = pd.DataFrame.from_dict(multi_label_bc_type_counts, orient=\"index\", columns=[\"multi-label-files\"])\n",
    "bc_types_distribution[\"multi-label-files-perc\"] = bc_types_distribution[\"multi-label-files\"] / files * 100\n",
    "bc_types_distribution = bc_types_distribution.sort_values(\"multi-label-files\", ascending=False)\n",
    "\n",
    "# Single-label files\n",
    "single_label_df = pd.DataFrame.from_dict(single_label_counts, orient=\"index\", columns=[\"single-label-files\"])\n",
    "single_label_df[\"single-label-files-perc\"] = single_label_df[\"single-label-files\"] / files * 100\n",
    "\n",
    "# Join both dataframes and fill missing values with zeros\n",
    "bc_types_distribution = bc_types_distribution.join(single_label_df, how=\"left\", on=None, validate=\"many_to_many\")\n",
    "bc_types_distribution[\"single-label-files\"] = bc_types_distribution[\"single-label-files\"].fillna(0).astype(int)\n",
    "bc_types_distribution[\"single-label-files-perc\"] = bc_types_distribution[\"single-label-files-perc\"].fillna(0)\n",
    "\n",
    "bc_types_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "ax = sns.barplot(\n",
    "    x=bc_types_distribution.index, \n",
    "    y=bc_types_distribution[\"multi-label-files\"]\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Annotations\n",
    "for p in ax.patches:\n",
    "    p.set_alpha(0.8)\n",
    "    height = p.get_height() + 1\n",
    "    ax.annotate(f'{int(height)}',\n",
    "                (p.get_x() + p.get_width() / 2., height),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "# Labels\n",
    "ax.set_title(\"Distribution of Breaking Change Types\")\n",
    "ax.set_xlabel(\"Breaking change type\")\n",
    "ax.set_ylabel(\"File count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4443e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "fig, ax = plt.subplots(figsize=(max(6, len(bc_types_distribution) * 0.6), 4))\n",
    "x = np.arange(len(bc_types_distribution))\n",
    "width = 0.45\n",
    "ax.bar(x - width / 2, bc_types_distribution['multi-label-files'], width, label='Multi-label files')\n",
    "ax.bar(x + width / 2, bc_types_distribution['single-label-files'], width, label='Single-label files')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bc_types_distribution.index, rotation=90)\n",
    "\n",
    "# Annotations\n",
    "for p in ax.patches:\n",
    "    p.set_alpha(0.8)\n",
    "    height = p.get_height() + 1\n",
    "    ax.annotate(f'{int(height)}',\n",
    "                (p.get_x() + p.get_width() / 2., height),\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Labels\n",
    "ax.set_title(\"Distribution of Breaking Change Types\")\n",
    "ax.set_xlabel(\"Breaking change type\")\n",
    "ax.set_ylabel(\"File count\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59387b13",
   "metadata": {},
   "source": [
    "#### Remove files with NONE as BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ceb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_before = len(df)\n",
    "df = df[df[\"BC_kinds\"] != \"NONE\"]\n",
    "len_after = len(df)\n",
    "print(f\"Removed {len_before - len_after} files with BC_kinds NONE. Remaining files: {len_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7b343",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "**Current situation**\n",
    "- Each file has more than one BC type, resulting in a multi-labeled dataset.\n",
    "- We should prevent label imbalance when performing the split. This is more challenging given the multi-labeled nature of the dataset.\n",
    "- We need to ensure we end up witha representative coverage of all BC types available in the dataset for bot the training and test datasets. \n",
    "- We do not cover all possible BC types due to the current dataset we count on. This should be reported in the threats to validity.\n",
    "\n",
    "**Alternatives to perform the split**\n",
    "- Random sampling with a final check would be possible if the dataset was not that small (as it is our case).\n",
    "- Multi-label stratified split: guarantees that each split preserves the proportion of labels (aka. BC types) in the training and test datasets. We account for the multi-label nature of the dataset---a simple stratified split could fail.\n",
    "- Multi-label stratified k-fold or cross-validation: prevents us from focusing on \"too good\" or \"too bad\" cases. Although ideal, it might become too computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fcd0e0",
   "metadata": {},
   "source": [
    "#### Multi-label stratified split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = MultiLabelBinarizer()\n",
    "Y = binarizer.fit_transform(df[\"bc_types\"])\n",
    "\n",
    "splitter = MultilabelStratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_idx, test_idx = next(splitter.split(df, Y))\n",
    "\n",
    "train_df = df.iloc[train_idx]\n",
    "test_df = df.iloc[test_idx]\n",
    "remaining_df = df.drop(test_df.index)\n",
    "\n",
    "print(f\"Size of training set: {len(train_df)} files\")\n",
    "print(f\"Size of test set: {len(test_df)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"data/test_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975ff07",
   "metadata": {},
   "source": [
    "#### Split the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569405aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_training_set(base_df, size, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly samples a subset of the training data.\n",
    "    \n",
    "    :param base_df: The DataFrame to sample from.\n",
    "    :param size: The number of samples to draw.\n",
    "    :param seed: The random seed for reproducibility.\n",
    "    :return: A DataFrame containing the sampled training data.\n",
    "    \"\"\"\n",
    "    return base_df.sample(n=size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6fbd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_label_distribution(df):\n",
    "    \"\"\"\n",
    "    Computes the distribution of breaking change types in the given \n",
    "    DataFrame.\n",
    "    \n",
    "    :param df: The DataFrame containing a \"bc_types\" column with lists \n",
    "        of breaking change types.\n",
    "    :return: A Counter object mapping each breaking change type to its count.\n",
    "    \"\"\"\n",
    "    return Counter([\",\".join(sorted(labels)) for labels in df[\"bc_types\"]])\n",
    "\n",
    "\n",
    "def is_training_dataset_balanced(train_df, test_df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs a chi-squared test to determine if the distribution of \n",
    "    breaking change types in the training set is statistically similar \n",
    "    to that in the test set.\n",
    "\n",
    "    :param train_df: The training DataFrame.\n",
    "    :param test_df: The test DataFrame.\n",
    "    :param alpha: The significance level for the test (default is 0.05).\n",
    "    :return: A tuple (is_balanced, p_value, chi2_statistic, cramers_v) where \n",
    "        is_balanced is True if the distributions are similar, False otherwise.\"\"\"\n",
    "    train_dist = compute_label_distribution(train_df)\n",
    "    test_dist = compute_label_distribution(test_df)\n",
    "\n",
    "    contingency = pd.DataFrame({\n",
    "        \"train\": train_dist,\n",
    "        \"test\": test_dist\n",
    "    }).fillna(0)\n",
    "\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "    # Calculate Cramér's V (effect size)\n",
    "    n = contingency.to_numpy().sum()\n",
    "    cramers_v = np.sqrt(chi2 / (n * (min(contingency.shape) - 1)))\n",
    "\n",
    "    return p_value >= alpha, p_value, chi2, cramers_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc093588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_sample_size(previous_sample_size: int, sample_increment: int, total_train: int) -> int:\n",
    "    \"\"\"\n",
    "    Computes the next sample size for the training dataset based on the previous sample size,\n",
    "    the sample increment, and the total number of training samples available.\n",
    "\n",
    "    :param previous_sample_size: The size of the previous training dataset.\n",
    "    :param sample_increment: The number of additional samples to add in the next round.\n",
    "    :param total_train: The total number of training samples available.\n",
    "    :return: The computed sample size for the next training dataset.\n",
    "    \"\"\"\n",
    "    if previous_sample_size == 1:\n",
    "        return sample_increment\n",
    "    elif previous_sample_size + sample_increment > total_train:\n",
    "        return total_train\n",
    "    else:        \n",
    "        return previous_sample_size + sample_increment\n",
    "    \n",
    "\n",
    "def create_training_datasets(remaining_df=remaining_df, sample_increment=25, min_train_size=1, rounds=5):\n",
    "    \"\"\"\n",
    "    Creates multiple training datasets of increasing size and checks \n",
    "    if they are balanced with respect to the test set.\n",
    "    \n",
    "    :param remaining_df: The DataFrame to sample from for creating training datasets.\n",
    "    :param sample_increment: The number of additional samples to add in each round.\n",
    "    :param min_train_size: The minimum number of samples for teh training dataset.\n",
    "    :param rounds: The number of training datasets to create.\n",
    "    \"\"\"\n",
    "    total_train = len(remaining_df)\n",
    "    sample_size = min_train_size\n",
    "\n",
    "    while sample_size != total_train:\n",
    "        _round = 0\n",
    "\n",
    "        while _round < rounds:\n",
    "            print(f\"Creating training dataset for round {_round + 1} with {sample_size} samples...\")\n",
    "            train_df = sample_training_set(remaining_df, sample_size)\n",
    "            is_balanced, p_value, chi2, cramers_v = is_training_dataset_balanced(train_df, test_df, alpha=0.01)\n",
    "        \n",
    "            if not is_balanced:\n",
    "                print(f\"Dataset for round {_round + 1} is imbalanced (p-value: {p_value:.4f}). Skipping this round.\")\n",
    "                continue\n",
    "\n",
    "            train_df.to_csv(f\"data/train_{sample_size}_round_{_round}.csv\", index=False)\n",
    "            _round += 1\n",
    "            print(f\"Chi-squared: {chi2:.4f}\")\n",
    "            print(f\"P-value: {p_value:.4f}\")\n",
    "            print(f\"Cramér's V: {cramers_v:.4f}\")\n",
    "\n",
    "        print(f\"Completed round {_round}.\")\n",
    "        sample_size = compute_next_sample_size(sample_size, sample_increment, total_train)\n",
    "        print(f\"Moving to next round with {sample_size} samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525cd94e",
   "metadata": {},
   "source": [
    "#### BC-based training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_datasets_per_bc_type(remaining_df=remaining_df, min_samples_per_type=40):\n",
    "    \"\"\"\n",
    "    Creates training datasets where each round focuses on files containing \n",
    "    a specific breaking change type. For each BC type found in the remaining_df,\n",
    "    it creates a training dataset containing only files with that label.\n",
    "\n",
    "    :param remaining_df: The DataFrame to sample from for creating training datasets.\n",
    "    :param min_samples_per_type: Minimum number of samples required for a BC type to \n",
    "        be included.\n",
    "    \"\"\"\n",
    "    bc_types = {bc_type for bc_types in remaining_df[\"bc_types\"] for bc_type in bc_types}\n",
    "    bc_types = sorted(bc_types)\n",
    "    \n",
    "    for bc_type in bc_types:\n",
    "        train_df = remaining_df[remaining_df[\"bc_types\"].apply(\n",
    "            lambda x, bc_type=bc_type: isinstance(x, list) and len(x) == 1 and x[0] == bc_type\n",
    "        )]\n",
    "        \n",
    "        if len(train_df) < min_samples_per_type:\n",
    "            print(f\"Skipping BC type '{bc_type}', only {len(train_df)} samples found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Creating training dataset for round {bc_type} with {len(train_df)} samples...\")\n",
    "        train_df.to_csv(f\"data/train_{bc_type}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22db125",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_datasets_per_bc_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e2df2",
   "metadata": {},
   "source": [
    "## \\<EOF\\>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
