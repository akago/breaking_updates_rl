{
    "absolute_path_to_file_in_container": "/singer/singer-commons/src/main/java/com/pinterest/singer/loggingaudit/client/AuditEventKafkaSender.java",
    "errors": [
        {
            "line_number": 121,
            "message": "[ERROR] /singer/singer-commons/src/main/java/com/pinterest/singer/loggingaudit/client/AuditEventKafkaSender.java:[121,36] unreported exception org.apache.thrift.transport.TTransportException; must be caught or declared to be thrown",
            "additional_info": "",
            "file_name": "AuditEventKafkaSender.java",
            "uid": "15ed8d33-f556-5799-ad09-bbcb60c3fb8d"
        }
    ],
    "prompt": "Act as an Automatic Program Repair (APR) tool, reply only with code, without explanation. \nYou are specialized in breaking dependency updates, in which the failure is caused by an external dependency. \nTo solve the failure you can only work on the client code.\n\nAPI upgrade: libthrift 0.12.0->0.16.0\n\nthe following client code fails: \n'''java\npackage com.pinterest.singer.loggingaudit.client;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.LinkedBlockingDeque;\nimport java.util.concurrent.ThreadLocalRandom;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport com.pinterest.singer.loggingaudit.client.common.LoggingAuditClientMetrics;\nimport com.pinterest.singer.loggingaudit.thrift.LoggingAuditEvent;\nimport com.pinterest.singer.loggingaudit.thrift.LoggingAuditHeaders;\nimport com.pinterest.singer.loggingaudit.thrift.LoggingAuditStage;\nimport com.pinterest.singer.loggingaudit.thrift.configuration.KafkaSenderConfig;\nimport com.pinterest.singer.metrics.OpenTsdbMetricConverter;\nimport com.pinterest.singer.utils.CommonUtils;\nimport org.apache.kafka.clients.producer.Callback;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport org.apache.kafka.common.PartitionInfo;\nimport org.apache.thrift.TException;\nimport org.apache.thrift.TSerializer;\npublic class AuditEventKafkaSender implements LoggingAuditEventSender {\n    private static final Logger LOG = LoggerFactory.getLogger(AuditEventKafkaSender.class);\n\n    private static final int MAX_RETRIES_FOR_SELECTION_RANDOM_PARTITION = 10;\n\n    private static final int PARTITIONS_REFRESH_INTERVAL_IN_SECONDS = 30;\n\n    private static final int NUM_OF_PARTITIONS_TO_TRY_SENDING = 3;\n\n    private static final int DEQUEUE_WAIT_IN_SECONDS = 30;\n\n    private static final int THREAD_SLEEP_IN_SECONDS = 10;\n\n    private int stopGracePeriodInSeconds = 300;\n\n    private final LoggingAuditStage stage;\n\n    private final String host;\n\n    private final LinkedBlockingDeque<LoggingAuditEvent> queue;\n\n    private KafkaProducer<byte[], byte[]> kafkaProducer;\n\n    private TSerializer serializer = new TSerializer();\n\n    private AtomicBoolean cancelled = new AtomicBoolean(false);\n\n    private String topic;\n\n    private String name;\n\n    private Thread thread;\n\n    private List<PartitionInfo> partitionInfoList = new ArrayList<>();\n\n    private long lastTimeUpdate = -1;\n\n    private Set<Integer> badPartitions = ConcurrentHashMap.newKeySet();\n\n    private Map<LoggingAuditHeaders, Integer> eventTriedCount = new ConcurrentHashMap<>();\n\n    private int currentPartitionId = -1;\n\n    public AuditEventKafkaSender(KafkaSenderConfig config, LinkedBlockingDeque<LoggingAuditEvent> queue, LoggingAuditStage stage, String host, String name) {\n        this.topic = config.getTopic();\n        this.queue = queue;\n        this.stage = stage;\n        this.host = host;\n        this.name = name;\n        this.stopGracePeriodInSeconds = config.getStopGracePeriodInSeconds();\n        this.badPartitions.add(-1);\n    }\n\n    public KafkaProducer<byte[], byte[]> getKafkaProducer() {\n        return kafkaProducer;\n    }\n\n    public void setKafkaProducer(KafkaProducer<byte[], byte[]> kafkaProducer) {\n        this.kafkaProducer = kafkaProducer;\n    }\n\n    private void refreshPartitionIfNeeded() {\n        if ((System.currentTimeMillis() - lastTimeUpdate) > (1000 * PARTITIONS_REFRESH_INTERVAL_IN_SECONDS)) {\n            try {\n                badPartitions.clear();\n                badPartitions.add(-1);\n                partitionInfoList = this.kafkaProducer.partitionsFor(topic);\n                lastTimeUpdate = System.currentTimeMillis();\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_PARTITIONS_REFRESH_COUNT, 1, \"host=\" + host, \"stage=\" + stage.toString());\n            } catch (Exception e) {\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_PARTITIONS_REFRESH_ERROR, 1, \"host=\" + host, \"stage=\" + stage.toString());\n            }\n        }\n        resetCurrentPartitionIdIfNeeded();\n    }\n\n    private void resetCurrentPartitionIdIfNeeded() {\n        if (partitionInfoList.size() == 0) {\n            currentPartitionId = -1;\n            return;\n        }\n        if (badPartitions.contains(currentPartitionId)) {\n            int trial = 0;\n            while (trial < MAX_RETRIES_FOR_SELECTION_RANDOM_PARTITION) {\n                trial += 1;\n                int index = ThreadLocalRandom.current().nextInt(partitionInfoList.size());\n                int randomPartition = partitionInfoList.get(index).partition();\n                if (!badPartitions.contains(randomPartition)) {\n                    LOG.warn(\"Change current partition of audit event topic from {} to {}\", currentPartitionId, randomPartition);\n                    currentPartitionId = randomPartition;\n                    OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_CURRENT_PARTITION_RESET, 1, \"host=\" + host, \"stage=\" + stage.toString());\n                    return;\n                }\n            } \n            currentPartitionId = partitionInfoList.get(ThreadLocalRandom.current().nextInt(partitionInfoList.size())).partition();\n            LOG.warn(\"After {} trials, set current partition to {}\", MAX_RETRIES_FOR_SELECTION_RANDOM_PARTITION, currentPartitionId);\n        }\n    }\n\n    @Override\n    public void run() {\n        LoggingAuditEvent event = null;\n        ProducerRecord<byte[], byte[]> record;\n        byte[] value = null;\n        while (!cancelled.get()) {\n            try {\n                refreshPartitionIfNeeded();\n                if (currentPartitionId == (-1)) {\n                    Thread.sleep(100);\n                    continue;\n                }\n                event = queue.poll(DEQUEUE_WAIT_IN_SECONDS, TimeUnit.SECONDS);\n                if (event != null) {\n                    try {\n                        value = serializer.serialize(event);\n                        record = new ProducerRecord<>(this.topic, currentPartitionId, null, value);\n                        kafkaProducer.send(record, new KafkaProducerCallback(event, currentPartitionId));\n                    } catch (TException e) {\n                        LOG.debug(\"[{}] failed to construct ProducerRecord because of serialization exception.\", Thread.currentThread().getName(), e);\n                        OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_SERIALIZATION_EXCEPTION, 1, \"host=\" + host, \"stage=\" + stage.toString(), \"logName=\" + event.getLoggingAuditHeaders().getLogName());\n                        eventTriedCount.remove(event.getLoggingAuditHeaders());\n                    }\n                }\n            } catch (InterruptedException e) {\n                LOG.warn(\"[{}] got interrupted when polling the queue and while loop is ended!\", Thread.currentThread().getName(), e);\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_DEQUEUE_INTERRUPTED_EXCEPTION, 1, \"host=\" + host, \"stage=\" + stage.toString());\n                break;\n            } catch (Exception e) {\n                LOG.warn(\"Exit the while loop and finish the thread execution due to exception: \", e);\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_EXCEPTION, 1, \"host=\" + host, \"stage=\" + stage.toString());\n                break;\n            }\n        } \n    }\n\n    public class KafkaProducerCallback implements Callback {\n        private LoggingAuditEvent event;\n\n        private int partition;\n\n        public KafkaProducerCallback(LoggingAuditEvent event, int partition) {\n            this.event = event;\n            this.partition = partition;\n        }\n\n        public void checkAndEnqueueWhenSendFailed() {\n            badPartitions.add(this.partition);\n            OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_PARTITION_ERROR, 1, \"host=\" + host, \"stage=\" + stage.toString(), \"topic=\" + topic, \"partition=\" + this.partition);\n            Integer count = eventTriedCount.get(event.getLoggingAuditHeaders());\n            if (count == null) {\n                eventTriedCount.put(event.getLoggingAuditHeaders(), 1);\n                insertEvent(event);\n                OpenTsdbMetricConverter.gauge(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_EVENTS_RETRIED, eventTriedCount.size(), \"host=\" + host, \"stage=\" + stage.toString(), \"topic=\" + topic);\n            } else if (count >= NUM_OF_PARTITIONS_TO_TRY_SENDING) {\n                LOG.debug(\"Failed to send audit event after trying {} partitions. Drop event.\", count);\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_EVENTS_DROPPED, 1, \"host=\" + host, \"stage=\" + stage.toString(), \"logName=\" + event.getLoggingAuditHeaders().getLogName());\n                eventTriedCount.remove(event.getLoggingAuditHeaders());\n            } else {\n                eventTriedCount.put(event.getLoggingAuditHeaders(), count + 1);\n                insertEvent(event);\n            }\n        }\n\n        public void insertEvent(LoggingAuditEvent event) {\n            try {\n                boolean success = queue.offerFirst(event, 3, TimeUnit.SECONDS);\n                if (!success) {\n                    LOG.debug(\"Failed to enqueue LoggingAuditEvent at head of the queue when executing \" + \"producer send callback. Drop this event.\");\n                    eventTriedCount.remove(event.getLoggingAuditHeaders());\n                }\n            } catch (InterruptedException ex) {\n                LOG.debug(\"Enqueuing LoggingAuditEvent at head of the queue was interrupted in callback. \" + \"Drop this event\");\n                eventTriedCount.remove(event.getLoggingAuditHeaders());\n            }\n        }\n\n        @Override\n        public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n            try {\n                if (e == null) {\n                    OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_EVENTS_ACKED, 1, \"host=\" + host, \"stage=\" + stage.toString(), \"logName=\" + event.getLoggingAuditHeaders().getLogName());\n                    eventTriedCount.remove(event.getLoggingAuditHeaders());\n                    badPartitions.remove(recordMetadata.partition());\n                } else {\n                    checkAndEnqueueWhenSendFailed();\n                }\n            } catch (Throwable t) {\n                LOG.warn(\"Exception throws in the callback. Drop this event {}\", event, t);\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_CALLBACK_EXCEPTION, 1, \"host=\" + host, \"stage=\" + stage.toString(), \"topic=\" + topic);\n            }\n        }\n    }\n\n    public synchronized void start() {\n        if (this.thread == null) {\n            thread = new Thread(this);\n            thread.setDaemon(true);\n            thread.setName(name);\n            thread.start();\n            LOG.warn(\"[{}] created and started [{}] to let it dequeue LoggingAuditEvents and send to Kafka.\", Thread.currentThread().getName(), name);\n        }\n    }\n\n    public synchronized void stop() {\n        LOG.warn(\"[{}] waits up to {} seconds to let [{}] send out LoggingAuditEvents left in the queue if\" + \" any.\", Thread.currentThread().getName(), stopGracePeriodInSeconds, name);\n        int i = 0;\n        int numOfRounds = stopGracePeriodInSeconds / THREAD_SLEEP_IN_SECONDS;\n        while ((((queue.size() > 0) && (this.thread != null)) && thread.isAlive()) && (i < numOfRounds)) {\n            i += 1;\n            try {\n                Thread.sleep(THREAD_SLEEP_IN_SECONDS * 1000);\n                CommonUtils.reportQueueUsage(queue.size(), queue.remainingCapacity(), host, stage.toString());\n                LOG.info(\"In {} round, [{}] waited {} seconds and the current queue size is {}\", i, Thread.currentThread().getName(), THREAD_SLEEP_IN_SECONDS, queue.size());\n            } catch (InterruptedException e) {\n                LOG.warn(\"[{}] got interrupted while waiting for [{}] to send out LoggingAuditEvents left \" + \"in the queue.\", Thread.currentThread().getName(), name, e);\n            }\n        } \n        cancelled.set(true);\n        if ((this.thread != null) && thread.isAlive()) {\n            thread.interrupt();\n        }\n        try {\n            this.kafkaProducer.close();\n        } catch (Throwable t) {\n            LOG.warn(\"Exception is thrown while stopping {}.\", name, t);\n        }\n        LOG.warn(\"[{}] is stopped and the number of LoggingAuditEvents left in the queue is {}.\", name, queue.size());\n    }\n}\n\n'''\nthe error is triggered in the following specific lines in the previous code:\n  private TSerializer serializer = new TSerializer();\nwith the following error message:\n[ERROR] /singer/singer-commons/src/main/java/com/pinterest/singer/loggingaudit/client/AuditEventKafkaSender.java:[121,36] unreported exception org.apache.thrift.transport.TTransportException; must be caught or declared to be thrown\nThe error is caused by a change in the API of the dependency. The new library version includes the following changes:\nFormat: element | nature | kind\norg.apache.thrift.transport.TSaslClientTransport.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.TDeserializer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TIOStreamTransport.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TNonblockingTransport.<init> | DELETION | CONSTRUCTOR_REMOVED\norg.apache.thrift.TSerializer.toString | DELETION | METHOD_REMOVED\norg.apache.thrift.transport.AutoExpandingBufferReadTransport.<init> | DELETION | CONSTRUCTOR_REMOVED\norg.apache.thrift.transport.TSaslServerTransport.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TSocket.<init> | DELETION | CONSTRUCTOR_REMOVED\norg.apache.thrift.transport.AutoExpandingBufferWriteTransport.<init> | DELETION | CONSTRUCTOR_REMOVED\norg.apache.thrift.transport.TSocket.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.AutoExpandingBuffer.<init> | DELETION | CONSTRUCTOR_REMOVED\norg.apache.thrift.transport.TNonblockingSocket.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TMemoryInputTransport.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.server.AbstractNonblockingServer$AsyncFrameBuffer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TByteBuffer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.TSerializer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TMemoryBuffer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TZlibTransport.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\n\n- Identify the specific API changes that are causing the failure in the client code. \n- Compare the old and new API versions, noting any changes in method signatures, return types, or parameter lists. \n- Determine which parts of the client code need to be updated to accommodate these API changes. \n- Consider any constraints or requirements for the fix (e.g., not changing function signatures, potential import adjustments). \n- Plan the minimal set of changes needed to fix the issue while keeping the code functional and compliant with the new API. \n- Consider potential side effects of the proposed changes on other parts of the code. \n- Ensure that the planned changes will result in a complete and compilable class. \n- If applicable, note any additional imports that may be needed due to the API changes.  \n- Propose a patch that can be applied to the code to fix the issue. \n- Return only a complete and compilable class in a fenced code block. \n- You CANNOT change the function signature of any method but may create variables if it simplifies the code. \n- You CAN remove the @Override annotation IF AND ONLY IF the method no longer overrides a method in the updated dependency version. \n- If fixing the issue requires addressing missing imports, ensure the correct package or class is used in accordance with the newer dependency version. \n- Avoid removing any existing code unless it directly causes a compilation or functionality error. \n- Return only the fixed class, ensuring it fully compiles and adheres to these constraints.",
    "buggy_lines": "  private TSerializer serializer = new TSerializer();",
    "error_message": "[ERROR] /singer/singer-commons/src/main/java/com/pinterest/singer/loggingaudit/client/AuditEventKafkaSender.java:[121,36] unreported exception org.apache.thrift.transport.TTransportException; must be caught or declared to be thrown",
    "api_diff": "Format: element | nature | kind\norg.apache.thrift.transport.TSaslClientTransport.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.TDeserializer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TIOStreamTransport.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TNonblockingTransport.<init> | DELETION | CONSTRUCTOR_REMOVED\norg.apache.thrift.TSerializer.toString | DELETION | METHOD_REMOVED\norg.apache.thrift.transport.AutoExpandingBufferReadTransport.<init> | DELETION | CONSTRUCTOR_REMOVED\norg.apache.thrift.transport.TSaslServerTransport.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TSocket.<init> | DELETION | CONSTRUCTOR_REMOVED\norg.apache.thrift.transport.AutoExpandingBufferWriteTransport.<init> | DELETION | CONSTRUCTOR_REMOVED\norg.apache.thrift.transport.TSocket.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.AutoExpandingBuffer.<init> | DELETION | CONSTRUCTOR_REMOVED\norg.apache.thrift.transport.TNonblockingSocket.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TMemoryInputTransport.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.server.AbstractNonblockingServer$AsyncFrameBuffer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TByteBuffer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.TSerializer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TMemoryBuffer.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION\norg.apache.thrift.transport.TZlibTransport.<init> | MUTATION | METHOD_NOW_THROWS_CHECKED_EXCEPTION",
    "original_code": "package com.pinterest.singer.loggingaudit.client;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.LinkedBlockingDeque;\nimport java.util.concurrent.ThreadLocalRandom;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport com.pinterest.singer.loggingaudit.client.common.LoggingAuditClientMetrics;\nimport com.pinterest.singer.loggingaudit.thrift.LoggingAuditEvent;\nimport com.pinterest.singer.loggingaudit.thrift.LoggingAuditHeaders;\nimport com.pinterest.singer.loggingaudit.thrift.LoggingAuditStage;\nimport com.pinterest.singer.loggingaudit.thrift.configuration.KafkaSenderConfig;\nimport com.pinterest.singer.metrics.OpenTsdbMetricConverter;\nimport com.pinterest.singer.utils.CommonUtils;\nimport org.apache.kafka.clients.producer.Callback;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport org.apache.kafka.common.PartitionInfo;\nimport org.apache.thrift.TException;\nimport org.apache.thrift.TSerializer;\npublic class AuditEventKafkaSender implements LoggingAuditEventSender {\n    private static final Logger LOG = LoggerFactory.getLogger(AuditEventKafkaSender.class);\n\n    private static final int MAX_RETRIES_FOR_SELECTION_RANDOM_PARTITION = 10;\n\n    private static final int PARTITIONS_REFRESH_INTERVAL_IN_SECONDS = 30;\n\n    private static final int NUM_OF_PARTITIONS_TO_TRY_SENDING = 3;\n\n    private static final int DEQUEUE_WAIT_IN_SECONDS = 30;\n\n    private static final int THREAD_SLEEP_IN_SECONDS = 10;\n\n    private int stopGracePeriodInSeconds = 300;\n\n    private final LoggingAuditStage stage;\n\n    private final String host;\n\n    private final LinkedBlockingDeque<LoggingAuditEvent> queue;\n\n    private KafkaProducer<byte[], byte[]> kafkaProducer;\n\n    private TSerializer serializer = new TSerializer();\n\n    private AtomicBoolean cancelled = new AtomicBoolean(false);\n\n    private String topic;\n\n    private String name;\n\n    private Thread thread;\n\n    private List<PartitionInfo> partitionInfoList = new ArrayList<>();\n\n    private long lastTimeUpdate = -1;\n\n    private Set<Integer> badPartitions = ConcurrentHashMap.newKeySet();\n\n    private Map<LoggingAuditHeaders, Integer> eventTriedCount = new ConcurrentHashMap<>();\n\n    private int currentPartitionId = -1;\n\n    public AuditEventKafkaSender(KafkaSenderConfig config, LinkedBlockingDeque<LoggingAuditEvent> queue, LoggingAuditStage stage, String host, String name) {\n        this.topic = config.getTopic();\n        this.queue = queue;\n        this.stage = stage;\n        this.host = host;\n        this.name = name;\n        this.stopGracePeriodInSeconds = config.getStopGracePeriodInSeconds();\n        this.badPartitions.add(-1);\n    }\n\n    public KafkaProducer<byte[], byte[]> getKafkaProducer() {\n        return kafkaProducer;\n    }\n\n    public void setKafkaProducer(KafkaProducer<byte[], byte[]> kafkaProducer) {\n        this.kafkaProducer = kafkaProducer;\n    }\n\n    private void refreshPartitionIfNeeded() {\n        if ((System.currentTimeMillis() - lastTimeUpdate) > (1000 * PARTITIONS_REFRESH_INTERVAL_IN_SECONDS)) {\n            try {\n                badPartitions.clear();\n                badPartitions.add(-1);\n                partitionInfoList = this.kafkaProducer.partitionsFor(topic);\n                lastTimeUpdate = System.currentTimeMillis();\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_PARTITIONS_REFRESH_COUNT, 1, \"host=\" + host, \"stage=\" + stage.toString());\n            } catch (Exception e) {\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_PARTITIONS_REFRESH_ERROR, 1, \"host=\" + host, \"stage=\" + stage.toString());\n            }\n        }\n        resetCurrentPartitionIdIfNeeded();\n    }\n\n    private void resetCurrentPartitionIdIfNeeded() {\n        if (partitionInfoList.size() == 0) {\n            currentPartitionId = -1;\n            return;\n        }\n        if (badPartitions.contains(currentPartitionId)) {\n            int trial = 0;\n            while (trial < MAX_RETRIES_FOR_SELECTION_RANDOM_PARTITION) {\n                trial += 1;\n                int index = ThreadLocalRandom.current().nextInt(partitionInfoList.size());\n                int randomPartition = partitionInfoList.get(index).partition();\n                if (!badPartitions.contains(randomPartition)) {\n                    LOG.warn(\"Change current partition of audit event topic from {} to {}\", currentPartitionId, randomPartition);\n                    currentPartitionId = randomPartition;\n                    OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_CURRENT_PARTITION_RESET, 1, \"host=\" + host, \"stage=\" + stage.toString());\n                    return;\n                }\n            } \n            currentPartitionId = partitionInfoList.get(ThreadLocalRandom.current().nextInt(partitionInfoList.size())).partition();\n            LOG.warn(\"After {} trials, set current partition to {}\", MAX_RETRIES_FOR_SELECTION_RANDOM_PARTITION, currentPartitionId);\n        }\n    }\n\n    @Override\n    public void run() {\n        LoggingAuditEvent event = null;\n        ProducerRecord<byte[], byte[]> record;\n        byte[] value = null;\n        while (!cancelled.get()) {\n            try {\n                refreshPartitionIfNeeded();\n                if (currentPartitionId == (-1)) {\n                    Thread.sleep(100);\n                    continue;\n                }\n                event = queue.poll(DEQUEUE_WAIT_IN_SECONDS, TimeUnit.SECONDS);\n                if (event != null) {\n                    try {\n                        value = serializer.serialize(event);\n                        record = new ProducerRecord<>(this.topic, currentPartitionId, null, value);\n                        kafkaProducer.send(record, new KafkaProducerCallback(event, currentPartitionId));\n                    } catch (TException e) {\n                        LOG.debug(\"[{}] failed to construct ProducerRecord because of serialization exception.\", Thread.currentThread().getName(), e);\n                        OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_SERIALIZATION_EXCEPTION, 1, \"host=\" + host, \"stage=\" + stage.toString(), \"logName=\" + event.getLoggingAuditHeaders().getLogName());\n                        eventTriedCount.remove(event.getLoggingAuditHeaders());\n                    }\n                }\n            } catch (InterruptedException e) {\n                LOG.warn(\"[{}] got interrupted when polling the queue and while loop is ended!\", Thread.currentThread().getName(), e);\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_DEQUEUE_INTERRUPTED_EXCEPTION, 1, \"host=\" + host, \"stage=\" + stage.toString());\n                break;\n            } catch (Exception e) {\n                LOG.warn(\"Exit the while loop and finish the thread execution due to exception: \", e);\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_EXCEPTION, 1, \"host=\" + host, \"stage=\" + stage.toString());\n                break;\n            }\n        } \n    }\n\n    public class KafkaProducerCallback implements Callback {\n        private LoggingAuditEvent event;\n\n        private int partition;\n\n        public KafkaProducerCallback(LoggingAuditEvent event, int partition) {\n            this.event = event;\n            this.partition = partition;\n        }\n\n        public void checkAndEnqueueWhenSendFailed() {\n            badPartitions.add(this.partition);\n            OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_PARTITION_ERROR, 1, \"host=\" + host, \"stage=\" + stage.toString(), \"topic=\" + topic, \"partition=\" + this.partition);\n            Integer count = eventTriedCount.get(event.getLoggingAuditHeaders());\n            if (count == null) {\n                eventTriedCount.put(event.getLoggingAuditHeaders(), 1);\n                insertEvent(event);\n                OpenTsdbMetricConverter.gauge(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_EVENTS_RETRIED, eventTriedCount.size(), \"host=\" + host, \"stage=\" + stage.toString(), \"topic=\" + topic);\n            } else if (count >= NUM_OF_PARTITIONS_TO_TRY_SENDING) {\n                LOG.debug(\"Failed to send audit event after trying {} partitions. Drop event.\", count);\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_EVENTS_DROPPED, 1, \"host=\" + host, \"stage=\" + stage.toString(), \"logName=\" + event.getLoggingAuditHeaders().getLogName());\n                eventTriedCount.remove(event.getLoggingAuditHeaders());\n            } else {\n                eventTriedCount.put(event.getLoggingAuditHeaders(), count + 1);\n                insertEvent(event);\n            }\n        }\n\n        public void insertEvent(LoggingAuditEvent event) {\n            try {\n                boolean success = queue.offerFirst(event, 3, TimeUnit.SECONDS);\n                if (!success) {\n                    LOG.debug(\"Failed to enqueue LoggingAuditEvent at head of the queue when executing \" + \"producer send callback. Drop this event.\");\n                    eventTriedCount.remove(event.getLoggingAuditHeaders());\n                }\n            } catch (InterruptedException ex) {\n                LOG.debug(\"Enqueuing LoggingAuditEvent at head of the queue was interrupted in callback. \" + \"Drop this event\");\n                eventTriedCount.remove(event.getLoggingAuditHeaders());\n            }\n        }\n\n        @Override\n        public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n            try {\n                if (e == null) {\n                    OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_EVENTS_ACKED, 1, \"host=\" + host, \"stage=\" + stage.toString(), \"logName=\" + event.getLoggingAuditHeaders().getLogName());\n                    eventTriedCount.remove(event.getLoggingAuditHeaders());\n                    badPartitions.remove(recordMetadata.partition());\n                } else {\n                    checkAndEnqueueWhenSendFailed();\n                }\n            } catch (Throwable t) {\n                LOG.warn(\"Exception throws in the callback. Drop this event {}\", event, t);\n                OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_CALLBACK_EXCEPTION, 1, \"host=\" + host, \"stage=\" + stage.toString(), \"topic=\" + topic);\n            }\n        }\n    }\n\n    public synchronized void start() {\n        if (this.thread == null) {\n            thread = new Thread(this);\n            thread.setDaemon(true);\n            thread.setName(name);\n            thread.start();\n            LOG.warn(\"[{}] created and started [{}] to let it dequeue LoggingAuditEvents and send to Kafka.\", Thread.currentThread().getName(), name);\n        }\n    }\n\n    public synchronized void stop() {\n        LOG.warn(\"[{}] waits up to {} seconds to let [{}] send out LoggingAuditEvents left in the queue if\" + \" any.\", Thread.currentThread().getName(), stopGracePeriodInSeconds, name);\n        int i = 0;\n        int numOfRounds = stopGracePeriodInSeconds / THREAD_SLEEP_IN_SECONDS;\n        while ((((queue.size() > 0) && (this.thread != null)) && thread.isAlive()) && (i < numOfRounds)) {\n            i += 1;\n            try {\n                Thread.sleep(THREAD_SLEEP_IN_SECONDS * 1000);\n                CommonUtils.reportQueueUsage(queue.size(), queue.remainingCapacity(), host, stage.toString());\n                LOG.info(\"In {} round, [{}] waited {} seconds and the current queue size is {}\", i, Thread.currentThread().getName(), THREAD_SLEEP_IN_SECONDS, queue.size());\n            } catch (InterruptedException e) {\n                LOG.warn(\"[{}] got interrupted while waiting for [{}] to send out LoggingAuditEvents left \" + \"in the queue.\", Thread.currentThread().getName(), name, e);\n            }\n        } \n        cancelled.set(true);\n        if ((this.thread != null) && thread.isAlive()) {\n            thread.interrupt();\n        }\n        try {\n            this.kafkaProducer.close();\n        } catch (Throwable t) {\n            LOG.warn(\"Exception is thrown while stopping {}.\", name, t);\n        }\n        LOG.warn(\"[{}] is stopped and the number of LoggingAuditEvents left in the queue is {}.\", name, queue.size());\n    }\n}\n",
    "project": "singer",
    "libraryName": "libthrift",
    "libraryGroupID": "org.apache.thrift",
    "newVersion": "0.16.0",
    "previousVersion": "0.12.0",
    "breakingCommit": "067f5d2c81ff87c90755f4ed48f62eb5faa8ecf9"
}